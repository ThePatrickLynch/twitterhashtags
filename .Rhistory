mytext = gsub('[[:punct:]]', ' ', mytext)
# build a corpus
# VectorSource specifies that the source is character vectors.
myCorpus <- Corpus(VectorSource(mytext))
copyCorpus <- myCorpus
# myCorpus <- tm_map(myCorpus, PlainTextDocument)
# myCorpus <- tm_map(myCorpus, stemDocument, language="english")
# strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
#remove stopwords
myCorpus <- tm_map(myCorpus, removeWords, stopwords('english')) #remove stopwords
tdm = TermDocumentMatrix(myCorpus)
# term frequencies
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 3)
dm <- data.frame(term = names(term.freq), freq = term.freq)
# wordcloud
wordcloud(dm$term, dm$freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
ap_td <- tibble()
ap_td <- tidy(tweets$text)
View(ap_td)
ap_td <- tidy(tweets$text)
ap_sentiments <- ap_td %>% right_join(get_sentiments("nrc")) %>% filter(!is.na(sentiment)) %>% count(sentiment, sort=TRUE, by='x, word')
View(ap_td)
colnames(ap_td,"word")
View(ap_td)
COLNAMES(AP_TD)
colnames(ap_td)
colnames(ap_td,"word")
colnames(ap_td)
colnames(ap_td, 'word')
colnames(ap_td) <- 'word'
colnames(ap_td)
ap_sentiments <- ap_td %>% right_join(get_sentiments("nrc")) %>% filter(!is.na(sentiment)) %>% count(sentiment, sort=TRUE)
View(ap_sentiments)
View(ap_td)
nrcjoy <- get_sentiments("bing") %>% filter(sentiment == "joy")
View(nrcjoy)
nrcjoy <- get_sentiments("nrc") %>% filter(sentiment == "joy")
View(nrcjoy)
ap_sentiments <- ap_td %>% right_join(get_sentiments("bing")) %>% filter(!is.na(sentiment)) %>% count(sentiment, sort=TRUE)
ap_sentiments <- ap_td %>% right_join(get_sentiments("afinn")) %>% filter(!is.na(sentiment)) %>% count(sentiment, sort=TRUE)
ap_sentiments <- ap_td %>% right_join(get_sentiments("nrc")) %>% filter(!is.na(sentiment)) %>% count(sentiment, sort=TRUE)
get_sentiments("nrc")
ap_sentiments <- ap_td %>% right_join(get_sentiments("nrc"))
ap_sentiments <- ap_td %>% right_join(get_sentiments("nrc")) %>% filter(!is.na(sentiment)) %>% count(sentiment, sort=TRUE)
install.packages("plotly")
library(plotly)
p <- plot_ly(
x = ap_sentiments$sentiment,
y = ap_sentiments$n,
name = "Sentiments LKTHEChat ",
type = "bar"
)
chart_link = plotly_POST(p, filename="bar/basic")
chart_link
barplot(ap_sentiments)
ap_sentiments <- as.data.frame(ap_sentiments)
barplot(ap_sentiments)
ap_sentiments <- as.table(ap_sentiments)
barplot(ap_sentiments)
ap_sentiments <- ap_td %>% right_join(get_sentiments("nrc")) %>% filter(!is.na(sentiment)) %>% count(sentiment, sort=TRUE)
barplot(ap_sentiments$n)
barplot(ap_sentiments$n, names.arg = ap_sentiments$sentiment)
barplot(ap_sentiments$n, names.arg = ap_sentiments$sentiment, horiz = TRUE)
barplot(ap_sentiments$n)
barplot(ap_sentiments$n, xlab = sentiment)
barplot(ap_sentiments$n, xlab = sentiments$sentiment)
barplot(ap_sentiments$n, names.arg = sentiments$sentiment, xlab = "Sentiment, ylab = "Frequency"")
barplot(ap_sentiments$n, names.arg = sentiments$sentiment, xlab = "Sentiment, ylab = "Frequency")
barplot(ap_sentiments$n, names.arg = sentiments$sentiment, xlab = "Sentiment", ylab = "Frequency")
barplot(ap_sentiments$n, names.arg = sentiments$sentiment)
xlab
xlab
, names.arg = ap_sentiments$sentiment
barplot(ap_sentiments$n, names.arg = ap_sentiments$sentiment)
barplot(ap_sentiments$n, names.arg = ap_sentiments$sentiment, xlab = "Sentiment", ylab = "Frequency")
barplot(ap_sentiments$n, legend.text = ap_sentiments$sentiment, xlab = "Sentiment", ylab = "Frequency")
barplot(ap_sentiments$n, legend.text = ap_sentiments$sentiment, xlab = "Sentiment", ylab = "Frequency", col=heat.colors(10))
barplot(ap_sentiments$n, legend.text = ap_sentiments$sentiment, xlab = "Sentiment", ylab = "Frequency", col=colors)
colors <- ("Red", "Green", c(gray.colors(8)))
colors <- ("Red", "Green", gray.colors(8))
colors <- ("Red", "Green")
colors <- c("Red","Green", gray.colors(8))
barplot(ap_sentiments$n, legend.text = ap_sentiments$sentimentxlab = "Sentiment", ylab = "Frequency", col=colors)
barplot(ap_sentiments$n, legend.text = ap_sentiments$sentiment, xlab = "Sentiment", ylab = "Frequency", col=colors)
colors <- c("Red","Green", "lightblue", "mistyrose", "lightcyan", "lavender", "pink", "brown", "teal")
barplot(ap_sentiments$n, , xlab = "Sentiment", ylab = "Frequency", col=colors)
colors <- c("Red","Green", "lightblue", "mistyrose", "lightcyan", "lavender", "pink", "brown", "green")
barplot(ap_sentiments$n, , xlab = "Sentiment", ylab = "Frequency", col=colors)
colors <- c("Red","Green", col=cm.colors(8))
barplot(ap_sentiments$n, , xlab = "Sentiment", ylab = "Frequency", col=colors)
barplot(ap_sentiments$n, names.arg = sentiments$sentiment, xlab = "Sentiment", ylab = "Frequency", col=colors)
barplot(ap_sentiments$n, names.arg = ap_sentiments$sentiment, xlab = "Sentiment", ylab = "Frequency", col=colors)
barplot(ap_sentiments$n, names.arg = ap_sentiments$sentiment, xlab = "Sentiment", ylab = "Frequency", col=colors, horiz = TRUE)
barplot(ap_sentiments$n, names.arg = ap_sentiments$sentiment, xlab = "Sentiment", ylab = "Frequency", col=colors, horiz = TRUE, las=3)
barplot(ap_sentiments$n, names.arg = ap_sentiments$sentiment, xlab = "Sentiment", ylab = "Frequency", col=colors, las=3)
barplot(ap_sentiments$n, names.arg = ap_sentiments$sentiment, ylab = "Frequency", col=colors, las=3)
View(tweets)
tweets <- read.csv("lthechat 4-4 to 10-4-2017.csv")
barplot(ap_sentiments$n, names.arg = ap_sentiments$sentiment, ylab = "Frequency", col=colors, las=3, main = "#LTHEchat 4-9 April 2017")
View(dm)
View(ap_td)
View(poster.freq)
library(twitteR)
library(wordcloud)
library(stringr)
library(tm)
library(longurl)
library(wordcloud)
library(RColorBrewer)
library(SnowballC)
library(dplyr)
library(tidytext)
library(tidyr)
library(plotly)
#setup_twitter_oauth("dr83qJt5IfcuqmicXPI9yINlA", "c0bSVElsvFtuHQnlZhnvphup98486t1Qm3BJEezTqIlNfSzvM6","37933003-LSHwa6XzUtCwXnt3HN4nw0cq37Qd8ALtnyTEAYsI3", "hXrLrYqKkzmoqyaZJsfmTI5bO5zv3yPVytR9fMDWVuSpl")
#setwd("d:/Data/github/R Twitter")
#rdmTweets <- searchTwitter("#lthechat", n=1500, since='2017-4-4')
#n <- length(rdmTweets)
#tweets <- do.call("rbind", lapply(rdmTweets, as.data.frame)) # convert to a datafame
# write.csv(tweets, file="lthechat 4-4 to 10-4-2017.csv") # save for posterity
tweets <- read.csv("lthechat 4-4 to 10-4-2017.csv")
posters <- sort(unique(tweets$screenName))
n.posters <- length(posters)
# counts per poster
poster.freq <- sort(table(tweets$screenName), decreasing = TRUE)
poster.freq <- as.data.frame(poster.freq) # coerce to dataframe
url_pattern <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
for(i in 1:n.posters) {
this.poster <- poster.freq$Var1[i]
subbit <- subset(tweets, tweets$screenName == this.poster)
numtweets <- length(subbit)
poster.freq[i,3] <- sum(str_count(subbit$text, "http")) # number of urls linked by user
poster.freq[i,4] <- sum(subbit$favoriteCount) # number of times posts were favorited
poster.freq[i,5] <- sum(str_count(subbit$text, "@")) # number of others referred
poster.freq[i,6] <- sum(subbit$retweetCount) # number of times posts were retweeted
poster.freq[i,7] <- sum(subbit$isRetweet, na.rm=TRUE) # number of posts that were retweeted
repliesto <- sum( !is.na( subbit$replyToSN)) # number in reply to
replylthe <- length(which(subbit$replyToSN == "LTHEchat")) # don't count
poster.freq[i,8] <- repliesto - replylthe
#  textURL <- na.omit(str_extract(subbit$text, url_pattern))
#  textURL2 <- expand_urls(textURL)
#  textURL3 <- as.list(textURL2$expanded_url)
}
colnames(poster.freq) <- c("Poster","Posts made","Links embedded","Favouriting count", "Included others", "Total retweets", "Posts retweeted", "In reply to !lthe")
######################################################
# mess with content of posts
######################################################
# get rid of posts by LTHEchat - i'm not interested in counting the time the question is asked
subbit <- subset(tweets, tweets$screenName != "LTHEchat")
mytext <- subbit$text
#################
# clean
#################
#remove retweet entries
mytext = gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', mytext)
#remove emoticons
mytext <- iconv(mytext, "latin1", "ASCII", "")
# remove control characters
mytext <- str_replace_all(mytext, "[[:cntrl:]]", " ")
# remove &amp
mytext <- str_replace_all(mytext, "&amp", " ")
# remove links
mytext <- str_replace_all(mytext, "(http[^ ]*)|(www\\.[^ ]*)", " ")
# convert tweets to lower case
mytext <- tolower(mytext)
# remove at people
mytext = gsub('@\\w+', '', mytext)
# remove tags
mytext <-gsub("#[[:alnum:][:punct:]]*","",mytext)
# remove punctuation
mytext = gsub('[[:punct:]]', ' ', mytext)
# build a corpus
# VectorSource specifies that the source is character vectors.
myCorpus <- Corpus(VectorSource(mytext))
copyCorpus <- myCorpus
# myCorpus <- tm_map(myCorpus, PlainTextDocument)
# myCorpus <- tm_map(myCorpus, stemDocument, language="english")
# strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
#remove stopwords
myCorpus <- tm_map(myCorpus, removeWords, stopwords('english')) #remove stopwords
tdm = TermDocumentMatrix(myCorpus)
# term frequencies
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 3)
dm <- data.frame(term = names(term.freq), freq = term.freq)
# wordcloud
wordcloud(dm$term, dm$freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
##################
# sentiment
##################
ap_td <- tibble()
ap_td <- tidy(tweets$text)
colnames(ap_td) <- "word"
ap_sentiments <- ap_td %>% right_join(get_sentiments("nrc")) %>% filter(!is.na(sentiment)) %>% count(sentiment, sort=TRUE)
#ap_sentiments <- as.table(ap_sentiments)
colors <- c("Red","Green", col=cm.colors(8))
barplot(ap_sentiments$n, names.arg = ap_sentiments$sentiment, ylab = "Frequency", col=colors, las=3, main = "#LTHEchat 4-9 April 2017")
#legend.text = ap_sentiments$sentiment
term.freq <- subset(term.freq, term.freq >= 10)
dm <- data.frame(term = names(term.freq), freq = term.freq)
# wordcloud
wordcloud(dm$term, dm$freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
term.freq <- subset(term.freq, term.freq >= 20)
dm <- data.frame(term = names(term.freq), freq = term.freq)
# wordcloud
wordcloud(dm$term, dm$freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 20)
dm <- data.frame(term = names(term.freq), freq = term.freq)
# wordcloud
wordcloud(dm$term, dm$freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
myCorpus <- Corpus(VectorSource(mytext))
copyCorpus <- myCorpus
# myCorpus <- tm_map(myCorpus, PlainTextDocument)
# myCorpus <- tm_map(myCorpus, stemDocument, language="english")
# strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
#remove stopwords
myCorpus <- tm_map(myCorpus, removeWords, stopwords('english')) #remove stopwords
tdm = TermDocumentMatrix(myCorpus)
gsub("\\d", "", "001a Frozen Niagara Entrance")
#remove retweet entries
mytext = gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', mytext)
#remove emoticons
mytext <- iconv(mytext, "latin1", "ASCII", "")
# remove control characters
mytext <- str_replace_all(mytext, "[[:cntrl:]]", " ")
# remove &amp
mytext <- str_replace_all(mytext, "&amp", " ")
# remove links
mytext <- str_replace_all(mytext, "(http[^ ]*)|(www\\.[^ ]*)", " ")
# convert tweets to lower case
mytext <- tolower(mytext)
# remove at people
mytext = gsub('@\\w+', '', mytext)
# remove tags
mytext <-gsub("#[[:alnum:][:punct:]]*","",mytext)
# remove punctuation
mytext = gsub('[[:punct:]]', ' ', mytext)
# remove numbers
mytext <- gsub("\\d", "", mytext)
# build a corpus
# VectorSource specifies that the source is character vectors.
myCorpus <- Corpus(VectorSource(mytext))
copyCorpus <- myCorpus
# myCorpus <- tm_map(myCorpus, PlainTextDocument)
# myCorpus <- tm_map(myCorpus, stemDocument, language="english")
# strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
#remove stopwords
myCorpus <- tm_map(myCorpus, removeWords, stopwords('english')) #remove stopwords
tdm = TermDocumentMatrix(myCorpus)
#Retrieving Text from Twitter
rm(list=ls())
#Twitter API requires authentication since March 2013. Please follow instructions in "Section 3 - Authentication with OAuth" in the twitteR vignettes on # CRAN or this link to complete authentication before running the code below.
library(twitteR)
library(wordcloud)
library(stringr)
library(tm)
library(longurl)
library(wordcloud)
library(RColorBrewer)
library(SnowballC)
library(dplyr)
library(tidytext)
library(tidyr)
library(plotly)
#setup_twitter_oauth("dr83qJt5IfcuqmicXPI9yINlA", "c0bSVElsvFtuHQnlZhnvphup98486t1Qm3BJEezTqIlNfSzvM6","37933003-LSHwa6XzUtCwXnt3HN4nw0cq37Qd8ALtnyTEAYsI3", "hXrLrYqKkzmoqyaZJsfmTI5bO5zv3yPVytR9fMDWVuSpl")
#setwd("d:/Data/github/R Twitter")
#rdmTweets <- searchTwitter("#lthechat", n=1500, since='2017-4-4')
#n <- length(rdmTweets)
#tweets <- do.call("rbind", lapply(rdmTweets, as.data.frame)) # convert to a datafame
# write.csv(tweets, file="lthechat 4-4 to 10-4-2017.csv") # save for posterity
tweets <- read.csv("lthechat 4-4 to 10-4-2017.csv")
posters <- sort(unique(tweets$screenName))
n.posters <- length(posters)
# counts per poster
poster.freq <- sort(table(tweets$screenName), decreasing = TRUE)
poster.freq <- as.data.frame(poster.freq) # coerce to dataframe
url_pattern <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
for(i in 1:n.posters) {
this.poster <- poster.freq$Var1[i]
subbit <- subset(tweets, tweets$screenName == this.poster)
numtweets <- length(subbit)
poster.freq[i,3] <- sum(str_count(subbit$text, "http")) # number of urls linked by user
poster.freq[i,4] <- sum(subbit$favoriteCount) # number of times posts were favorited
poster.freq[i,5] <- sum(str_count(subbit$text, "@")) # number of others referred
poster.freq[i,6] <- sum(subbit$retweetCount) # number of times posts were retweeted
poster.freq[i,7] <- sum(subbit$isRetweet, na.rm=TRUE) # number of posts that were retweeted
repliesto <- sum( !is.na( subbit$replyToSN)) # number in reply to
replylthe <- length(which(subbit$replyToSN == "LTHEchat")) # don't count
poster.freq[i,8] <- repliesto - replylthe
#  textURL <- na.omit(str_extract(subbit$text, url_pattern))
#  textURL2 <- expand_urls(textURL)
#  textURL3 <- as.list(textURL2$expanded_url)
}
colnames(poster.freq) <- c("Poster","Posts made","Links embedded","Favouriting count", "Included others", "Total retweets", "Posts retweeted", "In reply to !lthe")
######################################################
# mess with content of posts
######################################################
# get rid of posts by LTHEchat - i'm not interested in counting the time the question is asked
subbit <- subset(tweets, tweets$screenName != "LTHEchat")
mytext <- subbit$text
#################
# clean
#################
#remove retweet entries
mytext = gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', mytext)
#remove emoticons
mytext <- iconv(mytext, "latin1", "ASCII", "")
# remove control characters
mytext <- str_replace_all(mytext, "[[:cntrl:]]", " ")
# remove &amp
mytext <- str_replace_all(mytext, "&amp", " ")
# remove links
mytext <- str_replace_all(mytext, "(http[^ ]*)|(www\\.[^ ]*)", " ")
# convert tweets to lower case
mytext <- tolower(mytext)
# remove at people
mytext = gsub('@\\w+', '', mytext)
# remove tags
mytext <-gsub("#[[:alnum:][:punct:]]*","",mytext)
# remove punctuation
mytext = gsub('[[:punct:]]', ' ', mytext)
# remove numbers
mytext <- gsub("\\d", "", mytext)
# build a corpus
# VectorSource specifies that the source is character vectors.
myCorpus <- Corpus(VectorSource(mytext))
copyCorpus <- myCorpus
# myCorpus <- tm_map(myCorpus, PlainTextDocument)
# myCorpus <- tm_map(myCorpus, stemDocument, language="english")
# strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
#remove stopwords
myCorpus <- tm_map(myCorpus, removeWords, stopwords('english')) #remove stopwords
tdm = TermDocumentMatrix(myCorpus)
# term frequencies
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 20)
dm <- data.frame(term = names(term.freq), freq = term.freq)
# wordcloud
wordcloud(dm$term, dm$freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
##################
# sentiment
##################
ap_td <- tibble()
ap_td <- tidy(tweets$text)
colnames(ap_td) <- "word"
ap_sentiments <- ap_td %>% right_join(get_sentiments("nrc")) %>% filter(!is.na(sentiment)) %>% count(sentiment, sort=TRUE)
#ap_sentiments <- as.table(ap_sentiments)
colors <- c("Red","Green", col=cm.colors(8))
barplot(ap_sentiments$n, names.arg = ap_sentiments$sentiment, ylab = "Frequency", col=colors, las=3, main = "#LTHEchat 4-9 April 2017")
barplot(ap_sentiments$n, names.arg = ap_sentiments$sentiment, ylab = "Frequency", col=colors, las=3, main = "#LTHEchat 4-9 April 2017",legend.text = ap_sentiments$sentiment)
barplot(ap_sentiments$n, ylab = "Frequency", col=colors, las=3, main = "#LTHEchat 4-9 April 2017",legend.text = ap_sentiments$sentiment)
View(ap_sentiments)
rm(list=ls()) # just clear everything
setwd("D:/canvas/unpackedFiles")
myFile <- "requests_001(725).txt"
outFile <- "requests processed"
thisoutFile <- "requests_001(725).csv"
remove <- c(1,3:5,8:9,16,22,24:27)
rec.skip <- 0                   # starting point
rec.chunksize <- 10       # number of records in each chunk
canvasrequest <- read.table(myFile, sep='\t', na.strings="\\N", quote='"', fill=TRUE)
canvasrequest <- canvasrequest[ -c(remove)] # strip unwanted columns
colnames(canvasrequest) = c("timestamp", "user_id", "course_id", "quiz_id", "discussion_id", "conversation_id",
"assignment_id", "url", "user_agent", "remote_ip","interaction_micros", "web_application_controller",
"web_application_action", "web_application_context_type","real_user_id")
# strip unwanted rows
canvasrequest <- subset(canvasrequest, !is.na(canvasrequest$user_id)) # no user identified
canvasrequest <- subset(canvasrequest, canvasrequest$web_application_controller!="accounts") # not account management
canvasrequest <- subset(canvasrequest, canvasrequest$web_application_action!="masquerade") # not changing to masquerade
canvasrequest$real_user_id <- NULL # not masquerades
write.table(canvasrequest, thisoutFile, sep=",", row.names = FALSE, col.names = FALSE, append = FALSE)
i <- 1
for (i in 1:1){
# Read next chunk
rec.skip <- rec.skip + rec.chunksize + 1
thisoutFile <- capture.output(cat(outFile, i, ".csv"))    # new file for each chunk
canvasrequest <- read.table(myFile, sep='\t', na.strings="\\N", quote='"', fill=TRUE, skip=rec.skip, nrows=rec.chunksize)
#colnames(canvasrequest) = c("timestamp", "user_id", "course_id", "quiz_id", "discussion_id", "conversation_id",
#                        "assignment_id", "url", "user_agent", "remote_ip","interaction_micros", "web_application_controller",
#                        "web_applicaiton_action", "web_application_context_type","real_user_id")
#  canvasrequest <- canvasrequest[ -c(remove)] # strip unwanted
# write.table(canvasrequest, thisoutFile, sep=",", row_names = FALSE, col.names = FALSE, append = TRUE)
}
######
#
# Read canvas requests file and strip uneeded fields to reduce size
# processed in 3M chunks
#
######
rm(list=ls()) # just clear everything
setwd("D:/canvas/unpackedFiles")
myFile <- "requests.txt"
outFile <- "requests processed"
thisoutFile <- "processed.csv"
remove <- c(1,3:5,8:9,16,22,24:27)
rec.skip <- 0                   # starting point
rec.chunksize <- 10       # number of records in each chunk
canvasrequest <- read.table(myFile, sep='\t', na.strings="\\N", quote='"', fill=TRUE)
#Retrieving Text from Twitter
rm(list=ls())
#Twitter API requires authentication since March 2013. Please follow instructions in "Section 3 - Authentication with OAuth" in the twitteR vignettes on # CRAN or this link to complete authentication before running the code below.
library(twitteR)
library(wordcloud)
library(stringr)
library(tm)
library(longurl)
library(wordcloud)
library(RColorBrewer)
library(SnowballC)
library(dplyr)
library(tidytext)
library(tidyr)
library(plotly)
#setup_twitter_oauth("dr83qJt5IfcuqmicXPI9yINlA", "c0bSVElsvFtuHQnlZhnvphup98486t1Qm3BJEezTqIlNfSzvM6","37933003-LSHwa6XzUtCwXnt3HN4nw0cq37Qd8ALtnyTEAYsI3", "hXrLrYqKkzmoqyaZJsfmTI5bO5zv3yPVytR9fMDWVuSpl")
#setwd("d:/Data/github/R Twitter")
#rdmTweets <- searchTwitter("#lthechat", n=1500, since='2017-10-01')
#n <- length(rdmTweets)
#tweets <- do.call("rbind", lapply(rdmTweets, as.data.frame)) # convert to a datafame
#Retrieving Text from Twitter
rm(list=ls())
#Twitter API requires authentication since March 2013. Please follow instructions in "Section 3 - Authentication with OAuth" in the twitteR vignettes on # CRAN or this link to complete authentication before running the code below.
library(twitteR)
library(wordcloud)
library(stringr)
library(tm)
library(longurl)
library(wordcloud)
library(RColorBrewer)
library(SnowballC)
library(dplyr)
library(tidytext)
library(tidyr)
library(plotly)
setup_twitter_oauth("dr83qJt5IfcuqmicXPI9yINlA", "c0bSVElsvFtuHQnlZhnvphup98486t1Qm3BJEezTqIlNfSzvM6","37933003-LSHwa6XzUtCwXnt3HN4nw0cq37Qd8ALtnyTEAYsI3", "hXrLrYqKkzmoqyaZJsfmTI5bO5zv3yPVytR9fMDWVuSpl")
setwd("d:/Data/github/R Twitter")
rdmTweets <- searchTwitter("#lthechat", n=1500, since='2017-10-01')
n <- length(rdmTweets)
tweets <- do.call("rbind", lapply(rdmTweets, as.data.frame)) # convert to a datafame
View(rdmTweets)
View(tweets)
#Retrieving Text from Twitter
rm(list=ls())
#Twitter API requires authentication since March 2013. Please follow instructions in "Section 3 - Authentication with OAuth" in the twitteR vignettes on # CRAN or this link to complete authentication before running the code below.
library(twitteR)
library(wordcloud)
library(stringr)
library(tm)
library(longurl)
library(wordcloud)
library(RColorBrewer)
library(SnowballC)
library(dplyr)
library(tidytext)
library(tidyr)
library(plotly)
setup_twitter_oauth("dr83qJt5IfcuqmicXPI9yINlA", "c0bSVElsvFtuHQnlZhnvphup98486t1Qm3BJEezTqIlNfSzvM6","37933003-LSHwa6XzUtCwXnt3HN4nw0cq37Qd8ALtnyTEAYsI3", "hXrLrYqKkzmoqyaZJsfmTI5bO5zv3yPVytR9fMDWVuSpl")
setwd("d:/Data/github/R Twitter")
rdmTweets <- searchTwitter("#lthechat", n=1500, since='2017-10-09')
n <- length(rdmTweets)
tweets <- do.call("rbind", lapply(rdmTweets, as.data.frame)) # convert to a datafame
View(tweets)
posters <- sort(unique(tweets$screenName))
n.posters <- length(posters)
poster.freq <- sort(table(tweets$screenName), decreasing = TRUE)
poster.freq <- as.data.frame(poster.freq) # coerce to dataframe
View(poster.freq)
url_pattern <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
for(i in 1:n.posters) {
this.poster <- poster.freq$Var1[i]
subbit <- subset(tweets, tweets$screenName == this.poster)
numtweets <- length(subbit)
poster.freq[i,3] <- sum(str_count(subbit$text, "http")) # number of urls linked by user
poster.freq[i,4] <- sum(subbit$favoriteCount) # number of times posts were favorited
poster.freq[i,5] <- sum(str_count(subbit$text, "@")) # number of others referred
poster.freq[i,6] <- sum(subbit$retweetCount) # number of times posts were retweeted
poster.freq[i,7] <- sum(subbit$isRetweet, na.rm=TRUE) # number of posts that were retweeted
repliesto <- sum( !is.na( subbit$replyToSN)) # number in reply to
replylthe <- length(which(subbit$replyToSN == "LTHEchat")) # don't count
poster.freq[i,8] <- repliesto - replylthe
#  textURL <- na.omit(str_extract(subbit$text, url_pattern))
#  textURL2 <- expand_urls(textURL)
#  textURL3 <- as.list(textURL2$expanded_url)
}
View(subbit)
View(poster.freq)
for(i in 1:n.posters) {
this.poster <- poster.freq$Var1[i]
subbit <- subset(tweets, tweets$screenName == this.poster)
numtweets <- length(subbit)
poster.freq[i,3] <- sum(str_count(subbit$text, "http")) # number of urls linked by user
poster.freq[i,4] <- sum(subbit$favoriteCount) # number of times posts were favorited
poster.freq[i,5] <- sum(str_count(subbit$text, "@")) # number of others referred
poster.freq[i,6] <- sum(subbit$retweetCount) # number of times posts were retweeted
poster.freq[i,7] <- sum(subbit$isRetweet, na.rm=TRUE) # number of posts that were retweeted
names(poster.freq) <- c("AQcreenName","Tweets", "URLs Linked","Favourited", "Refs to others","Retweeted by", "Retweeted by others" )
repliesto <- sum( !is.na( subbit$replyToSN)) # number in reply to
replylthe <- length(which(subbit$replyToSN == "LTHEchat")) # don't count
poster.freq[i,8] <- repliesto - replylthe
#  textURL <- na.omit(str_extract(subbit$text, url_pattern))
#  textURL2 <- expand_urls(textURL)
#  textURL3 <- as.list(textURL2$expanded_url)
}
View(tweets)
subbit <- subset(tweets, tweets$screenName != "LTHEchat")
mytext <- subbit$text
